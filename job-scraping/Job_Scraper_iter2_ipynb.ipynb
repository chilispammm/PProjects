{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linkedin Scraper"
      ],
      "metadata": {
        "id": "Rg93mzW2kELY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Imports and constants"
      ],
      "metadata": {
        "id": "HbmxpyKnZlwv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd1wWmqNWs3N"
      },
      "outputs": [],
      "source": [
        "import requests  # Web requests.\n",
        "from bs4 import BeautifulSoup  # HTML/XML parsing.\n",
        "import pandas as pd  # Data manipulation.\n",
        "import time  # Time-related functions.\n",
        "import random  # Random numbers.\n",
        "from typing import List, Dict, Optional  # Enables type hinting for improved code readability and maintainability.\n",
        "\n",
        "# User-Agent pool to rotate requests\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "USER_AGENTS = [\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0'\n",
        "]\n",
        "\n",
        "# Targeted African locations\n",
        "AFRICAN_LOCATIONS = [\n",
        "    \"Nairobi\", \"Kenya\",\n",
        "    \"Lagos\", \"Nigeria\",\n",
        "    \"Johannesburg\", \"South Africa\",\n",
        "    \"Cairo\", \"Egypt\",\n",
        "    \"Accra\", \"Ghana\",\n",
        "    \"Kampala\", \"Uganda\",\n",
        "    \"Dar es Salaam\", \"Tanzania\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Rate Limiting & User Agent Rotation"
      ],
      "metadata": {
        "id": "QG6zsJV8ayVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_delay() -> float:\n",
        "    \"\"\"Return a random delay between requests to avoid rate limiting\"\"\"\n",
        "    return random.uniform(1, 3)\n",
        "\n",
        "def get_random_user_agent() -> str:\n",
        "    \"\"\"Return a random user agent to rotate headers\"\"\"\n",
        "    return random.choice(USER_AGENTS)\n"
      ],
      "metadata": {
        "id": "RX_DFjoba13U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. URL Construction & Page Fetching"
      ],
      "metadata": {
        "id": "1dt9Ug-Pa4WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_search_url(title: str, location: str, start: int = 0) -> str:\n",
        "    \"\"\"Construct the LinkedIn job search URL\"\"\"\n",
        "    return f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={title}&location={location}&start={start}\"\n",
        "\n",
        "def fetch_job_listings(url: str) -> Optional[BeautifulSoup]:\n",
        "    \"\"\"Fetch job listings page with random delays and user agents\"\"\"\n",
        "    headers = {'User-Agent': get_random_user_agent()}\n",
        "    try:\n",
        "        time.sleep(get_random_delay())\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        return BeautifulSoup(response.text, \"html.parser\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching job listings: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "ylypf67Ia6oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Extract Job IDs from Listing Page"
      ],
      "metadata": {
        "id": "gh1V9uwXbMcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_job_ids(soup: BeautifulSoup) -> List[str]:\n",
        "    \"\"\"Extract job IDs from the listings page\"\"\"\n",
        "    id_list = []\n",
        "    for job in soup.find_all(\"li\"):\n",
        "        base_card_div = job.find(\"div\", {\"class\": \"base-card\"})\n",
        "        if base_card_div and \"data-entity-urn\" in base_card_div.attrs:\n",
        "            job_id = base_card_div.get(\"data-entity-urn\").split(\":\")[3]\n",
        "            id_list.append(job_id)\n",
        "    return id_list\n"
      ],
      "metadata": {
        "id": "mGGHkBDtbNzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Fetch and Parse Individual Job Postings"
      ],
      "metadata": {
        "id": "d28IwkFzbQ80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_job_details(job_id: str) -> Optional[Dict]:\n",
        "    \"\"\"Fetch detailed information for a single job posting\"\"\"\n",
        "    job_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}\"\n",
        "    headers = {'User-Agent': get_random_user_agent()}\n",
        "    try:\n",
        "        time.sleep(get_random_delay())\n",
        "        response = requests.get(job_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        job_soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        job_post = {\n",
        "            \"job_title\": extract_text(job_soup, \"h2\", {\"class\": \"top-card-layout__title\"}),\n",
        "            \"company_name\": extract_text(job_soup, \"a\", {\"class\": \"topcard__org-name-link\"}),\n",
        "            \"location\": extract_text(job_soup, \"span\", {\"class\": \"topcard__flavor--bullet\"}),\n",
        "            \"time_posted\": extract_text(job_soup, \"span\", {\"class\": \"posted-time-ago__text\"}),\n",
        "            \"num_applicants\": extract_text(job_soup, \"span\", {\"class\": \"num-applicants__caption\"}),\n",
        "            \"job_description\": extract_text(job_soup, \"div\", {\"class\": \"description__text\"}),\n",
        "            \"employment_type\": extract_text(job_soup, \"span\", {\"class\": \"description__job-criteria-text\"}, 1),\n",
        "            \"seniority_level\": extract_text(job_soup, \"span\", {\"class\": \"description__job-criteria-text\"}, 0),\n",
        "            \"job_function\": extract_text(job_soup, \"span\", {\"class\": \"description__job-criteria-text\"}, 2),\n",
        "            \"industries\": extract_text(job_soup, \"span\", {\"class\": \"description__job-criteria-text\"}, 3),\n",
        "            \"scraped_location\": location,\n",
        "            \"scraped_at\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "        return job_post\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching job details for ID {job_id}: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "2Cyu-Q1sbVjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Utility Function for Safe Text Extraction"
      ],
      "metadata": {
        "id": "_nUeml6HbW7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(soup: BeautifulSoup, tag: str, attrs: Dict, index: int = None) -> Optional[str]:\n",
        "    \"\"\"Helper function to safely extract text from HTML elements\"\"\"\n",
        "    try:\n",
        "        elements = soup.find_all(tag, attrs)\n",
        "        if elements:\n",
        "            if index is not None and len(elements) > index:\n",
        "                return elements[index].text.strip()\n",
        "            return elements[0].text.strip()\n",
        "    except:\n",
        "        pass\n",
        "    return None"
      ],
      "metadata": {
        "id": "oZKMY6tubZG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Full Scraping Routine per Location\n",
        "\n"
      ],
      "metadata": {
        "id": "IDHbzuq2bbjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_linkedin_jobs(title: str, location: str, pages: int = 3) -> pd.DataFrame:\n",
        "    \"\"\"Main function to scrape LinkedIn jobs\"\"\"\n",
        "    all_jobs = []\n",
        "    for page in range(pages):\n",
        "        start = page * 25\n",
        "        url = construct_search_url(title, location, start)\n",
        "        soup = fetch_job_listings(url)\n",
        "        if not soup:\n",
        "            continue\n",
        "        job_ids = extract_job_ids(soup)\n",
        "        print(f\"Found {len(job_ids)} jobs on page {page + 1}\")\n",
        "        for job_id in job_ids:\n",
        "            job_details = fetch_job_details(job_id)\n",
        "            if job_details:\n",
        "                all_jobs.append(job_details)\n",
        "    return pd.DataFrame(all_jobs)"
      ],
      "metadata": {
        "id": "rpIEArrRbeT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Main Execution and Data Processing"
      ],
      "metadata": {
        "id": "ZKG7lBMJbhDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    job_title = \"Data Analyst\"\n",
        "    all_african_jobs = pd.DataFrame()\n",
        "\n",
        "    for location in AFRICAN_LOCATIONS:\n",
        "        print(f\"\\nScraping {job_title} jobs in {location}...\")\n",
        "        jobs_df = scrape_linkedin_jobs(job_title, location)\n",
        "        if not jobs_df.empty:\n",
        "            all_african_jobs = pd.concat([all_african_jobs, jobs_df], ignore_index=True)\n",
        "\n",
        "    if not all_african_jobs.empty:\n",
        "        # APP platform-specific enrichment\n",
        "        all_african_jobs['is_remote'] = all_african_jobs['job_description'].str.contains('remote|work from home', case=False, regex=True)\n",
        "        all_african_jobs['requires_sql'] = all_african_jobs['job_description'].str.contains('sql|mysql|postgresql', case=False, regex=True)\n",
        "        all_african_jobs['requires_python'] = all_african_jobs['job_description'].str.contains('python', case=False)\n",
        "        all_african_jobs['requires_powerbi'] = all_african_jobs['job_description'].str.contains('powerbi|power bi', case=False, regex=True)\n",
        "\n",
        "        # Save to CSV with timestamp\n",
        "        timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"APP_Data_Analyst_Jobs_Africa_{timestamp}.csv\"\n",
        "        all_african_jobs.to_csv(filename, index=False)\n",
        "        print(f\"\\nSuccessfully saved {len(all_african_jobs)} jobs to {filename}\")\n",
        "    else:\n",
        "        print(\"\\nNo jobs found for the given criteria.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAffpftMbj3p",
        "outputId": "6da6b792-80ad-4c30-b384-71e3bc7dc3d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scraping Data Analyst jobs in Nairobi...\n",
            "Found 9 jobs on page 1\n",
            "Found 10 jobs on page 2\n",
            "Found 10 jobs on page 3\n",
            "\n",
            "Scraping Data Analyst jobs in Kenya...\n",
            "Found 9 jobs on page 1\n",
            "Found 10 jobs on page 2\n",
            "Found 10 jobs on page 3\n",
            "\n",
            "Scraping Data Analyst jobs in Lagos...\n",
            "Found 10 jobs on page 1\n",
            "Found 10 jobs on page 2\n",
            "Found 10 jobs on page 3\n",
            "\n",
            "Scraping Data Analyst jobs in Nigeria...\n",
            "Found 10 jobs on page 1\n",
            "Found 10 jobs on page 2\n",
            "Found 10 jobs on page 3\n",
            "\n",
            "Scraping Data Analyst jobs in Johannesburg...\n",
            "Found 10 jobs on page 1\n",
            "Found 0 jobs on page 2\n",
            "Found 0 jobs on page 3\n",
            "\n",
            "Scraping Data Analyst jobs in South Africa...\n",
            "Found 10 jobs on page 1\n",
            "Found 10 jobs on page 2\n",
            "Found 10 jobs on page 3\n",
            "\n",
            "Scraping Data Analyst jobs in Cairo...\n",
            "Found 10 jobs on page 1\n",
            "Found 10 jobs on page 2\n",
            "Found 10 jobs on page 3\n",
            "\n",
            "Scraping Data Analyst jobs in Egypt...\n",
            "Found 10 jobs on page 1\n",
            "Found 10 jobs on page 2\n",
            "Found 10 jobs on page 3\n",
            "\n",
            "Scraping Data Analyst jobs in Accra...\n",
            "Found 10 jobs on page 1\n",
            "Found 10 jobs on page 2\n",
            "Found 10 jobs on page 3\n",
            "\n",
            "Scraping Data Analyst jobs in Ghana...\n",
            "Found 10 jobs on page 1\n",
            "Found 10 jobs on page 2\n",
            "Found 10 jobs on page 3\n",
            "\n",
            "Scraping Data Analyst jobs in Kampala...\n",
            "Found 10 jobs on page 1\n",
            "Found 0 jobs on page 2\n",
            "Found 0 jobs on page 3\n",
            "\n",
            "Scraping Data Analyst jobs in Uganda...\n",
            "Found 10 jobs on page 1\n",
            "Found 1 jobs on page 2\n",
            "Found 0 jobs on page 3\n",
            "\n",
            "Scraping Data Analyst jobs in Dar es Salaam...\n",
            "Found 7 jobs on page 1\n",
            "Found 0 jobs on page 2\n",
            "Found 0 jobs on page 3\n",
            "\n",
            "Scraping Data Analyst jobs in Tanzania...\n",
            "Found 10 jobs on page 1\n",
            "Found 9 jobs on page 2\n",
            "Found 0 jobs on page 3\n",
            "\n",
            "Successfully saved 325 jobs to APP_Data_Analyst_Jobs_Africa_20250519_073915.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Brighter Monday Jobs"
      ],
      "metadata": {
        "id": "LGvBjuMbkHsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions to safely extract text\n",
        "def scrape_brightermonday(job_title: str, max_pages: int = 3) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scrape Data Analyst jobs from BrighterMonday Kenya\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.brightermonday.co.ke\"\n",
        "    search_url = f\"{base_url}/jobs?q={job_title.replace(' ', '+')}\"\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    all_jobs = []\n",
        "\n",
        "    for page in range(1, max_pages + 1):\n",
        "        try:\n",
        "            time.sleep(random.uniform(1, 3))\n",
        "            url = f\"{search_url}&page={page}\" if page > 1 else search_url\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            job_cards = soup.find_all('article', class_='search-result')\n",
        "\n",
        "            if not job_cards:\n",
        "                break\n",
        "\n",
        "            for card in job_cards:\n",
        "                job = {\n",
        "                    'source': 'BrighterMonday',\n",
        "                    'scraped_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                    'job_title': get_text(card, 'h3', 'search-result__job-title'),\n",
        "                    'company': get_text(card, 'div', 'search-result__job-meta'),\n",
        "                    'location': get_text(card, 'div', 'search-result__location'),\n",
        "                    'date_posted': get_text(card, 'div', 'search-result__job-age'),\n",
        "                    'job_url': base_url + card.find('a')['href'] if card.find('a') else None,\n",
        "                }\n",
        "\n",
        "                # Get additional details from job page\n",
        "                if job['job_url']:\n",
        "                    try:\n",
        "                        time.sleep(random.uniform(1, 2))\n",
        "                        detail_response = requests.get(job['job_url'], headers=headers)\n",
        "                        detail_soup = BeautifulSoup(detail_response.text, 'html.parser')\n",
        "\n",
        "                        job['job_description'] = get_text(detail_soup, 'div', 'description__content')\n",
        "                        job['employment_type'] = get_text(detail_soup, 'span', 'description__job-criteria-text', 0)\n",
        "                        job['experience_level'] = get_text(detail_soup, 'span', 'description__job-criteria-text', 1)\n",
        "                        job['salary'] = get_text(detail_soup, 'span', 'description__job-criteria-text', 2)\n",
        "\n",
        "                        # Skill detection\n",
        "                        desc = job.get('job_description', '').lower()\n",
        "                        job['is_remote'] = 'remote' in desc or 'work from home' in desc\n",
        "                        job['requires_sql'] = 'sql' in desc or 'mysql' in desc or 'postgresql' in desc\n",
        "                        job['requires_python'] = 'python' in desc\n",
        "                        job['requires_powerbi'] = 'power bi' in desc or 'powerbi' in desc\n",
        "                        job['requires_excel'] = 'excel' in desc\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error scraping job details: {e}\")\n",
        "\n",
        "                all_jobs.append(job)\n",
        "\n",
        "            print(f\"Scraped page {page} of BrighterMonday\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping BrighterMonday page {page}: {e}\")\n",
        "            break\n",
        "\n",
        "    return pd.DataFrame(all_jobs)\n",
        "\n",
        "def get_text(soup, tag: str, class_name: str, index: int = None) -> str:\n",
        "    \"\"\"Helper to safely extract text\"\"\"\n",
        "    elements = soup.find_all(tag, class_=class_name)\n",
        "    if elements:\n",
        "        if index is not None and len(elements) > index:\n",
        "            return elements[index].get_text(strip=True)\n",
        "        return elements[0].get_text(strip=True)\n",
        "    return None"
      ],
      "metadata": {
        "id": "K3u75xPkkvYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fuzu Scraper"
      ],
      "metadata": {
        "id": "araQAomKlqFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_fuzu(job_title: str, max_pages: int = 3) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scrape Data Analyst jobs from Fuzu Kenya\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.fuzu.com\"\n",
        "    search_url = f\"{base_url}/kenya/job?q={job_title.replace(' ', '+')}\"\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept-Language': 'en-US,en;q=0.9',\n",
        "    }\n",
        "\n",
        "    all_jobs = []\n",
        "\n",
        "    for page in range(1, max_pages + 1):\n",
        "        try:\n",
        "            time.sleep(random.uniform(1, 3))\n",
        "            url = f\"{search_url}&page={page}\" if page > 1 else search_url\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            job_cards = soup.find_all('div', class_='job-card')\n",
        "\n",
        "            if not job_cards:\n",
        "                break\n",
        "\n",
        "            for card in job_cards:\n",
        "                job = {\n",
        "                    'source': 'Fuzu',\n",
        "                    'scraped_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                    'job_title': get_text(card, 'h3', 'job-card__title'),\n",
        "                    'company': get_text(card, 'div', 'job-card__company'),\n",
        "                    'location': get_text(card, 'div', 'job-card__location'),\n",
        "                    'date_posted': get_text(card, 'div', 'job-card__date'),\n",
        "                    'job_url': base_url + card.find('a')['href'] if card.find('a') else None,\n",
        "                }\n",
        "\n",
        "                # Get additional details from job page\n",
        "                if job['job_url']:\n",
        "                    try:\n",
        "                        time.sleep(random.uniform(1, 2))\n",
        "                        detail_response = requests.get(job['job_url'], headers=headers)\n",
        "                        detail_soup = BeautifulSoup(detail_response.text, 'html.parser')\n",
        "\n",
        "                        job['job_description'] = get_text(detail_soup, 'div', 'job-description')\n",
        "                        job['requirements'] = get_text(detail_soup, 'div', 'job-requirements')\n",
        "\n",
        "                        # Extract salary if available\n",
        "                        salary_div = detail_soup.find('div', class_='salary-range')\n",
        "                        if salary_div:\n",
        "                            job['salary'] = salary_div.get_text(strip=True).replace('Salary:', '').strip()\n",
        "\n",
        "                        # Skill detection\n",
        "                        desc = (job.get('job_description', '') + job.get('requirements', '')).lower()\n",
        "                        job['is_remote'] = 'remote' in desc or 'work from home' in desc\n",
        "                        job['requires_sql'] = 'sql' in desc or 'mysql' in desc or 'postgresql' in desc\n",
        "                        job['requires_python'] = 'python' in desc\n",
        "                        job['requires_powerbi'] = 'power bi' in desc or 'powerbi' in desc\n",
        "                        job['requires_excel'] = 'excel' in desc\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error scraping job details: {e}\")\n",
        "\n",
        "                all_jobs.append(job)\n",
        "\n",
        "            print(f\"Scraped page {page} of Fuzu\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping Fuzu page {page}: {e}\")\n",
        "            break\n",
        "\n",
        "    return pd.DataFrame(all_jobs)"
      ],
      "metadata": {
        "id": "YVZGWcyrlruH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combined Execution for Brighter monday and Fuzu"
      ],
      "metadata": {
        "id": "-Dh6AevJlv5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def scrape_all_kenyan_job_sites():\n",
        "    \"\"\"Scrape all platforms and combine results\"\"\"\n",
        "    job_title = \"Data Analyst\"\n",
        "\n",
        "    print(\"Starting BrighterMonday scrape...\")\n",
        "    bm_df = scrape_brightermonday(job_title)\n",
        "\n",
        "    print(\"\\nStarting Fuzu scrape...\")\n",
        "    fuzu_df = scrape_fuzu(job_title)\n",
        "\n",
        "    print(\"\\nStarting LinkedIn Kenya scrape...\")\n",
        "    linkedin_df = scrape_linkedin_jobs(job_title, \"Kenya\")\n",
        "\n",
        "    # Combine all dataframes\n",
        "    combined_df = pd.concat([bm_df, fuzu_df, linkedin_df], ignore_index=True)\n",
        "\n",
        "    # Save to CSV\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"APP_Kenya_Data_Analyst_Jobs_{timestamp}.csv\"\n",
        "    combined_df.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\"\\nScraping complete! Saved {len(combined_df)} jobs to {filename}\")\n",
        "    return combined_df\n",
        "\n",
        "# Run the combined scraper\n",
        "if __name__ == \"__main__\":\n",
        "    df = scrape_all_kenyan_job_sites()\n",
        "    print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPECQAjLlzvA",
        "outputId": "4dcddae6-318a-430f-a8b8-44b564d393fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting BrighterMonday scrape...\n",
            "\n",
            "Starting Fuzu scrape...\n",
            "Error scraping Fuzu page 1: 403 Client Error: Forbidden for url: https://www.fuzu.com/kenya/job?q=Data+Analyst\n",
            "\n",
            "Starting LinkedIn Kenya scrape...\n",
            "Found 9 jobs on page 1\n",
            "Found 10 jobs on page 2\n",
            "Found 10 jobs on page 3\n",
            "\n",
            "Scraping complete! Saved 29 jobs to APP_Kenya_Data_Analyst_Jobs_20250519_081402.csv\n",
            "                              job_title                  company_name  \\\n",
            "0                      SQL Data Analyst                 Emma of Torre   \n",
            "1                Marketing Data Analyst                    3Commas.io   \n",
            "2                        Data Scientist  ENGIE Energy Access (Africa)   \n",
            "3                   Junior Game Analyst                Gunzilla Games   \n",
            "4  Business Intelligence Analyst - 1570                  In All Media   \n",
            "\n",
            "                                       location   time_posted num_applicants  \\\n",
            "0                                         Kenya    3 days ago           None   \n",
            "1  Nzalae/ Nzawa locations, Kitui County, Kenya   1 month ago           None   \n",
            "2                         Nairobi County, Kenya   2 weeks ago  80 applicants   \n",
            "3  Nzalae/ Nzawa locations, Kitui County, Kenya   1 month ago           None   \n",
            "4  Nzalae/ Nzawa locations, Kitui County, Kenya  2 months ago           None   \n",
            "\n",
            "                                     job_description employment_type  \\\n",
            "0  I’m helping Confident LIMS find a top candidat...        Contract   \n",
            "1  3Commas is seeking a skilled Marketing Data An...       Full-time   \n",
            "2  Job Purpose/Mission This position will be part...       Full-time   \n",
            "3  About Gunzilla GamesFounded in 2020, Gunzilla ...       Temporary   \n",
            "4  Job Title: Business Intelligence Analyst (Fina...       Full-time   \n",
            "\n",
            "    seniority_level                                   job_function  \\\n",
            "0  Mid-Senior level   Analyst, Information Technology, and Science   \n",
            "1  Mid-Senior level                         Information Technology   \n",
            "2  Mid-Senior level              Analyst, Engineering, and Science   \n",
            "3       Entry level                 Business Development and Sales   \n",
            "4  Mid-Senior level  Research, Analyst, and Information Technology   \n",
            "\n",
            "                                          industries scraped_location  \\\n",
            "0                  Technology, Information and Media         Tanzania   \n",
            "1                               Software Development         Tanzania   \n",
            "2  Technology, Information and Media, Services fo...         Tanzania   \n",
            "3                                     Computer Games         Tanzania   \n",
            "4                  Information Technology & Services         Tanzania   \n",
            "\n",
            "            scraped_at  \n",
            "0  2025-05-19 08:12:54  \n",
            "1  2025-05-19 08:12:56  \n",
            "2  2025-05-19 08:12:59  \n",
            "3  2025-05-19 08:13:02  \n",
            "4  2025-05-19 08:13:04  \n"
          ]
        }
      ]
    }
  ]
}